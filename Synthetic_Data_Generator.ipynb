{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It89APiAtTUF"
   },
   "source": [
    "# Project Overview: Synthetic Data Generator\n",
    "\n",
    "The goal of this project is to\n",
    "*   Use models that can generate synthetic datasets\n",
    "*   Create a Gradio UI for the product\n",
    "\n",
    "Why this is useful\n",
    "*   Applies to almost any business area\n",
    "*   Can be used in day job and personal projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1r6AyvfNHwX"
   },
   "source": [
    "# STEP 0: Installs, Imports, API Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzor9pmnNYwN"
   },
   "source": [
    "## Installs\n",
    "Since we're using a Google Colab to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M-mTmXz9USNe",
    "outputId": "f3623da4-54c6-4251-ec71-cf765a1a78fd"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj8v6g7ENXgv"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FW8nl3XRFrz0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import threading\n",
    "import tempfile\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Tuple, Optional\n",
    "from google.colab import drive, userdata\n",
    "from IPython.display import Markdown, display, update_display\n",
    "\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, TextIteratorStreamer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6f7tf8xNa9d"
   },
   "source": [
    "## Global Constants & Model Config Class Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3D1_T0uG_Qh"
   },
   "outputs": [],
   "source": [
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "max_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVdqboTMizkP"
   },
   "outputs": [],
   "source": [
    "# @TODO: Add some more models to test later - ran out of free Colab compute when making this\n",
    "model_choice = LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CraDK4mIcTZe"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "  model_id: str = model_choice\n",
    "  max_new_tokens: int = max_tokens\n",
    "  do_sample: bool = True\n",
    "  temperature: float = 0.7\n",
    "  top_p: float = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ITb14kwNdb2"
   },
   "source": [
    "## Sign into HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYW8kQYtF-3L",
    "outputId": "a92c6ebd-7597-4f6b-e9dd-394394956ba8"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "if hf_token and hf_token.startswith(\"hf_\"):\n",
    "  print(\"HF key looks good so far\")\n",
    "else:\n",
    "  print(\"HF key is not set - please click the key in the left sidebar\")\n",
    "\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9llJ0YNbRLUX"
   },
   "source": [
    "# STEP 1: Set up Tokenizer, Quantization and Model in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8WVDNicP5eK"
   },
   "outputs": [],
   "source": [
    "def load_model(cfg: ModelConfig):\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_quant_type = \"nf4\"\n",
    "  )\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(cfg.model_id)\n",
    "  if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "  # Adding a try/except (except will need more RAM)\n",
    "  try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_id,\n",
    "        quantization_config = quant_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype = torch.float16\n",
    "    )\n",
    "  except RuntimeError as e:\n",
    "    print(f\"WARNING: 4-bit quantized loading failed, falling back to full precision. Error: {e}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GLpveo8cgzq"
   },
   "source": [
    "# STEP 2: Design the Prompt Engine\n",
    "Allows us to tell the LLM *what to do*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q8VHUWKcAAR"
   },
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piEMmcSfMH-O"
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an accurate synthetic data generator.\n",
    "Your sole task is to output valid data in the format specified by the user.\n",
    "\n",
    "Hard rules:\n",
    "- Output ONLY a valid JSON array (list of objects). No markdown, no code fences, no commentary.\n",
    "- Every object must contain exactly the requested fields (no extras).\n",
    "- Values must match the requested types.\n",
    "- Use realistic values and include variability.\n",
    "- If a field is constrained by the user (e.g., ranges, enums), obey it.\n",
    "\n",
    "The user will provide:\n",
    "- Dataset description\n",
    "- Exact schema (fields + types)\n",
    "- Number of rows\n",
    "\n",
    "Return ONLY the JSON array.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1MgHID9j_Bf"
   },
   "source": [
    "## Repair Prompt\n",
    "Fun easter egg... we actually use the LLM again to repair the JSON output if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69KmaFM8kGZj"
   },
   "outputs": [],
   "source": [
    "repair_prompt = \"\"\"You are a JSON repair tool.\n",
    "\n",
    "Task:\n",
    "- You will be given text that should be a JSON array (list of objects), but it may be invalid JSON.\n",
    "- Produce ONLY a valid JSON array that preserves the intended data.\n",
    "- No markdown, no commentary, no extra keys.\n",
    "- If any values are missing quotes or invalid, fix them minimally.\n",
    "\n",
    "Return ONLY the JSON array.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHtzpMiZQufw"
   },
   "source": [
    "## Text Generation Function using HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-ddAwAncQfZ"
   },
   "outputs": [],
   "source": [
    "def _chat_completion_hf(tokenizer, model, messages: List[Dict[str, str]], cfg: ModelConfig):\n",
    "  # Prepare inputs for the model based on the current messages history.\n",
    "  enc = tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      return_tensors=\"pt\",\n",
    "      padding = True,\n",
    "      truncation = True\n",
    "  )\n",
    "\n",
    "  # Device Handling\n",
    "  device = next(model.parameters()).device\n",
    "  # Full transparency - Claude helped me with this input to device code to be friendly with Colab. My interpretation is below...\n",
    "  \"\"\"\n",
    "  This section takes the encoded, tokenized input data and moves it to the appropriate computing device (usually a GPU).\n",
    "  The if statement checks if enc is a single PyTorch tensor\n",
    "    - If so, the tensor is moved to the device and the attention mask is created\n",
    "    - If not, it's assumed to be a dict-like object (BatchEncoding) which has the input_ids and attention mask as separate tensors.\n",
    "      - We iterate through each key-value pair and moves the tensor value (v) to the device\n",
    "  \"\"\"\n",
    "  if isinstance(enc, torch.Tensor):\n",
    "    input_ids = enc.to(device)\n",
    "    attention_mask = torch.ones_like(input_ids, device=device)\n",
    "    inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "  else:\n",
    "    # BatchEncoding / dict-like\n",
    "    inputs = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "  \"\"\"\n",
    "  We generate the text here using input_ids, attention_mask and the params from cfg.\n",
    "  Disabled gradient calculation here since we're not updating the model, we're just doing inference\n",
    "  \"\"\"\n",
    "  with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=cfg.max_new_tokens,\n",
    "            do_sample=cfg.do_sample,\n",
    "            temperature=cfg.temperature,\n",
    "            top_p=cfg.top_p,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "  # Decoded should have entire prompt + LLM assistant response\n",
    "  decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "  return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wt22rJFcYFW"
   },
   "source": [
    "# STEP 3: Core Data Generation Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHLA2dDCeeS1"
   },
   "source": [
    "## JSON Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEEGrEbseftL"
   },
   "outputs": [],
   "source": [
    "# Tries to extract the JSON array from LLM output by []\n",
    "def _extract_json_array(text: str) -> str:\n",
    "  start = text.find(\"[\")    # Find the first [\n",
    "  end = text.rfind(\"]\")     # Find the last ]\n",
    "  if start == -1 or end == -1 or end <= start:\n",
    "    raise ValueError(\"Could not find a JSON array in the LLM output\")\n",
    "  return text[start:end + 1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ktiT22kexW0"
   },
   "outputs": [],
   "source": [
    "# Parses the JSON array\n",
    "def _loads_json_array(text: str) -> List[Dict[str, Any]]:\n",
    "  # Should work if everything is normal but let's look for some potential erros\n",
    "  data = json.loads(text)\n",
    "  if not isinstance(data, list):\n",
    "    raise ValueError(\"Parsed JSON isn't a list\")\n",
    "  if any(not isinstance(x, dict) for x in data):\n",
    "    raise ValueError(\"JSON array doesn't contain objects\")\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUUnzgnofHHq"
   },
   "outputs": [],
   "source": [
    "# Basic Schema Validation (looks for missing keys or extra keys)\n",
    "def _validate_schema(rows: List[Dict[str, Any]], fields: List[Tuple[str, str]]) -> None:\n",
    "  expected_keys = [f for f, _t in fields]\n",
    "  expected_set = set(expected_keys)\n",
    "\n",
    "  for i, r in enumerate(rows):\n",
    "    keys = set(r.keys())\n",
    "    missing = expected_set - keys\n",
    "    extra = keys - expected_set\n",
    "    if missing:\n",
    "      raise ValueError(f\"Row {i} is missing keys: {sorted(missing)}\")\n",
    "    if extra:\n",
    "      raise ValueError(f\"Row {i} has extra keys: {sorted(extra)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVqHQYihfkFl"
   },
   "outputs": [],
   "source": [
    "# Parses schema lines to return a list of (field, type)\n",
    "def parse_schema(schema_text: str) -> List[Tuple[str, str]]:\n",
    "  fields: List[Tuple[str, str]] = []\n",
    "\n",
    "  for line in schema_text.strip().splitlines():\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "      continue\n",
    "    if \":\" not in line:\n",
    "      raise ValueError(f\"Invalid schema line (expected field:type): '{line}'\")\n",
    "\n",
    "    # Just splitting the field name and it's type\n",
    "    field, type_str = line.split(\":\", 1)\n",
    "    fields.append(\n",
    "        (field.strip(), type_str.strip().lower())\n",
    "    )\n",
    "\n",
    "  if not fields:\n",
    "    raise ValueError(\"No fields found in schema\")\n",
    "  return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMWrTNtPrNqb"
   },
   "source": [
    "## Core Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXxCTpaMhABF"
   },
   "outputs": [],
   "source": [
    "def generate_dataset_json(\n",
    "    description: str,\n",
    "    schema_text: str,\n",
    "    num_rows: int,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    cfg: ModelConfig\n",
    ") -> Tuple[str, Optional[pd.DataFrame], Optional[str]]:\n",
    "    try:\n",
    "      fields = parse_schema(schema_text)\n",
    "      if num_rows <1 or num_rows > 200:\n",
    "        raise ValueError(\"Number of rows must be between 1 and 200\")\n",
    "\n",
    "      schema_lines = \"\\n\".join([f\"- {f}: {t}\" for f, t in fields])\n",
    "\n",
    "      # Crafting the user prompt with all the inputs\n",
    "      user_prompt = f\"\"\"Dataset description:\n",
    "{description}\n",
    "\n",
    "Schema (field:type):\n",
    "{schema_lines}\n",
    "\n",
    "Number of rows: {num_rows}\n",
    "\n",
    "Return ONLY a JSON array of {num_rows} objects.\n",
    "\"\"\"\n",
    "\n",
    "      # Stateless messages per generation (so we don't bog down memory)\n",
    "      # We really don't need to maintain the state since the dataset generation should be independent each time\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": system_prompt},\n",
    "          {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ]\n",
    "\n",
    "      # Get the raw output from the model and then extract the JSON array\n",
    "      raw = _chat_completion_hf(tokenizer, model, messages, cfg)\n",
    "      candidate = _extract_json_array(raw)\n",
    "\n",
    "      # Extract the rows of data\n",
    "      try:\n",
    "        rows = _loads_json_array(candidate)\n",
    "      except Exception:\n",
    "        # Repair attempt using an LLM again!\n",
    "        # If you made it this far into my code, you'll see that we actually use an LLM to repair the JSOn if needed\n",
    "          repair_messages = [\n",
    "              {\"role\": \"system\", \"content\": repair_prompt},\n",
    "              {\"role\": \"user\", \"content\": candidate},\n",
    "          ]\n",
    "          repaired_raw = _chat_completion_hf(tokenizer, model, repair_messages, cfg)\n",
    "          repaired_candidate = _extract_json_array(repaired_raw)\n",
    "          rows = _loads_json_array(repaired_candidate)\n",
    "          candidate = repaired_candidate\n",
    "\n",
    "      # Enforce Length (@TODO: future work - add error handlign)\n",
    "      if len(rows) != num_rows:\n",
    "        rows = rows[:num_rows]  # Just trim for now\n",
    "\n",
    "      # Validate the schema\n",
    "      _validate_schema(rows, fields)\n",
    "\n",
    "      df = pd.DataFrame(rows)\n",
    "\n",
    "      # Write the CSV\n",
    "      tmpdir = tempfile.mkdtemp()\n",
    "      csv_path = os.path.join(tmpdir, \"synthetic_data.csv\")\n",
    "      df.to_csv(csv_path, index=False)\n",
    "\n",
    "      return rows, df, csv_path\n",
    "    except Exception as e:\n",
    "      return {\"Error\": str(e)}, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jLJBkRzQ2x3"
   },
   "source": [
    "# STEP 4: Gradio User Interface\n",
    "Wrap everything in Gradio for publication\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMVyZrehrTkU"
   },
   "source": [
    "## Examples\n",
    "*   Dataset Description\n",
    "*   Schema Definition\n",
    "*   Nunber of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwogkcLOm6-u"
   },
   "outputs": [],
   "source": [
    "nfl_example = [\n",
    "    \"Synthetic dataset for NFL quarterback game-by-game passing yards\",\n",
    "    \"full_name:str\\nteam_abbreviation:str\\nopponent_team_abbreviation:str\\ngame_date:date\\npassing_yards:int\\npass_attempts:int\\npass_completions:int\\npassing_touchdowns:int\\npassing_interceptions:int\",\n",
    "    3\n",
    "]\n",
    "\n",
    "nba_example = [\n",
    "    \"Synthetic dataset for NBA player game-by-game stat lines\",\n",
    "    \"full_name:str\\nteam_abbreviation:str\\nopponent_team_abbreviation:str\\ngame_date:date\\npoints:int\\nrebounds:int\\nassists:int\\nsteals:int\\npersonal_fouls:int\",\n",
    "    4\n",
    "]\n",
    "\n",
    "stock_example = [\n",
    "    \"Synthetic dataset for 5 different stocks\",\n",
    "    \"ticker:str\\nopen:float\\nhigh:float\\nlow:float\\nclose:float\\nvolume:int\\nmarket_cap:float\",\n",
    "    5\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syjQXvFnrVDQ"
   },
   "source": [
    "## Gradio App Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTd1kB_0kXY4"
   },
   "outputs": [],
   "source": [
    "def build_app(tokenizer, model, cfg: ModelConfig):\n",
    "  with gr.Blocks(title = \"Nikhil Gavini's Synthetic Data Generator (Open-Source LLM)\") as demo:\n",
    "    ## Basic Title and Description\n",
    "    gr.Markdown(\"## Nikhil Gavini's Synthetic Data Generator using HuggingFace LLMs\")\n",
    "    gr.Markdown(\n",
    "        \"Enter a dataset description, a schema (one `field:type` per line), and row count. \\n\"\n",
    "        \"The model returns JSON and a downloadable CSV.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    ## First Row is User freeform Input\n",
    "    with gr.Row():\n",
    "      description = gr.Textbox(\n",
    "          label = \"Dataset Description\",\n",
    "          lines = 4,\n",
    "          placeholder = \"Describe the dataset you want to generate\",\n",
    "      )\n",
    "      schema = gr.Textbox(\n",
    "          label = \"Schema (field:type per line)\",\n",
    "          lines = 6,\n",
    "          placeholder = \"Enter one field:type per line\",\n",
    "      )\n",
    "\n",
    "\n",
    "    ## Allows the user pick how many rows they want\n",
    "    num_rows = gr.Slider(\n",
    "        minimum = 1,\n",
    "        maximum = 200,\n",
    "        value = 5,\n",
    "        step = 1,\n",
    "        label = \"Number of rows\",\n",
    "    )\n",
    "\n",
    "    ## Button will have functionality linked to it\n",
    "    generate_button = gr.Button(\"Generate\")\n",
    "\n",
    "\n",
    "    ## Outputs (JSON, DataFrame Preview, CSV Download)\n",
    "    json_output = gr.JSON(\n",
    "        label = \"JSON Output\"\n",
    "    )\n",
    "\n",
    "    df_output = gr.Dataframe(\n",
    "        label = \"CSV Preview (up to 5 rows)\"\n",
    "    )\n",
    "\n",
    "    download_file = gr.File(\n",
    "        label = \"Download CSV\"\n",
    "    )\n",
    "\n",
    "\n",
    "    ## Generate the data in all forms\n",
    "    def on_generate(desc, sch, nr):\n",
    "      json_text, df, csv_path = generate_dataset_json(\n",
    "          desc, sch, int(nr), tokenizer, model, cfg\n",
    "      )\n",
    "      if df is None:\n",
    "        return json_text, None, None\n",
    "      return json_text, df.head(5), csv_path\n",
    "\n",
    "\n",
    "    ## Link functionality to button\n",
    "    generate_button.click(\n",
    "        fn = on_generate,\n",
    "        inputs = [description, schema, num_rows],\n",
    "        outputs = [json_output, df_output, download_file]\n",
    "    )\n",
    "\n",
    "\n",
    "    ## Examples for user to pick from\n",
    "    gr.Examples(\n",
    "        examples = [\n",
    "            nfl_example,\n",
    "            nba_example,\n",
    "            stock_example\n",
    "        ],\n",
    "        inputs = [description, schema, num_rows]\n",
    "    )\n",
    "\n",
    "  return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkOM6Q0FrXH-"
   },
   "source": [
    "# Final Step: Call the App!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630,
     "referenced_widgets": [
      "af2445c1346a430a828d91c6a25baee7",
      "fa8984bd5e2d46f4a6476c822a6e93a1",
      "9eb8d52fcf08490e8904a581ed6a52e4",
      "62464733cf6745ff86ecd717d8d7c089",
      "042893e56b5745a28fb39be103681f3e",
      "485b141e272346ecb5e33873514c6134",
      "0a119a0e55f647e497f25b37cd9d7f34",
      "848cc0399fd84c779f75e18c94bab47c",
      "591667d268424e9dbe36de7707989e55",
      "3d46a539083144288a3b789124b60c1e",
      "98951311cc0f41dfaedba046da3151ef"
     ]
    },
    "id": "D4YMkzBrpOBb",
    "outputId": "acba29e1-4f54-4434-95cc-7a5fcd702bcc"
   },
   "outputs": [],
   "source": [
    "cfg = ModelConfig()\n",
    "tokenizer, model = load_model(cfg)\n",
    "demo = build_app(tokenizer, model, cfg)\n",
    "demo.launch(share = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "042893e56b5745a28fb39be103681f3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a119a0e55f647e497f25b37cd9d7f34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d46a539083144288a3b789124b60c1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "485b141e272346ecb5e33873514c6134": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "591667d268424e9dbe36de7707989e55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "62464733cf6745ff86ecd717d8d7c089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d46a539083144288a3b789124b60c1e",
      "placeholder": "​",
      "style": "IPY_MODEL_98951311cc0f41dfaedba046da3151ef",
      "value": " 2/2 [00:27&lt;00:00, 12.46s/it]"
     }
    },
    "848cc0399fd84c779f75e18c94bab47c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98951311cc0f41dfaedba046da3151ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9eb8d52fcf08490e8904a581ed6a52e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_848cc0399fd84c779f75e18c94bab47c",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_591667d268424e9dbe36de7707989e55",
      "value": 2
     }
    },
    "af2445c1346a430a828d91c6a25baee7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa8984bd5e2d46f4a6476c822a6e93a1",
       "IPY_MODEL_9eb8d52fcf08490e8904a581ed6a52e4",
       "IPY_MODEL_62464733cf6745ff86ecd717d8d7c089"
      ],
      "layout": "IPY_MODEL_042893e56b5745a28fb39be103681f3e"
     }
    },
    "fa8984bd5e2d46f4a6476c822a6e93a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_485b141e272346ecb5e33873514c6134",
      "placeholder": "​",
      "style": "IPY_MODEL_0a119a0e55f647e497f25b37cd9d7f34",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
